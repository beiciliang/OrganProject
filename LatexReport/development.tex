
\chapter{Web app development}

\section{System requirements}

As the Organ Project aims to share the historically important heritage of the Henry Willis pipe organ with as wide an audience as possible, one way of meeting the requirement is to make a virtual tour featuring different aspects of the organ by a cross-platform application.

The goal of Organ Project is not only to honour the legacy of the pipe organ by keeping up a regular organ recital diary, but also to programme concerts right across the genre pool, breathing fresh life into the organ by having it performed in styles which have never previously been attempted on a mechanical organ. However, only the live audiences in these concerts can feel how versatile the organ can be. People who have never been to the Union Chapel may just simply know the organ is a church instrument. With the popularisation of digital devices such as laptop, smartphone and tablets, versatility of the organ can be reflected by an application in the devices, which provides an opportunity for people to learn more about the organ.

The main reason for making a virtual tour is because the organ at Union Chapel is deliberately hidden away behind ornate screens under the rose window when under construction. The console is directly behind the pulpit so that the congregation will not be distracted by the sight of an organist. The music played by the organ will be the focus during worship. Since the application concentrates on the organ itself, a virtual tour can lead users to know where different parts of the organ are located. Especially for people don't have access to the organ, a virtual tour can provide users with a more intuitive experience.

Different aspects of the organ refer to its history, console, pipes and hydraulics. The organ was designed and built specially for the size and acoustics of the new chapel building in 1877 by master organ builder Henry \enquote{Father} Willis. It is one of just two organs left in the United Kingdom, and the only one in England, with a fully working original hydraulic (water powered) blowing system, which can be used as an alternative to the electric blowers. The console shown in Figure~\ref{fig:console} has 3 manuals (the organ term for keyboards): swell, great and choir, plus full foot pedals. It has 37 speaking and 2 sound altering stops (these are the knobs on either side of the organ console). By drawing the stops the player engages sets of pipes, which fall into various categories of sound quality - diapasons, strings, flutes, reeds - and at various pitches and volumes. There is also an enclosed swell organ with a mechanism for opening the box, thus controlling the volume. In order to let users operate the console, the application should provide an interface which allows users to press the keyboard, draw the stops and hear different timbres.
\begin{figure}
\centerline{\epsfig{figure=images/console.png,width=0.8\textwidth}}
\caption{The console of the organ.} 
\label{fig:console}
\end{figure}

When the organ was restored in 2012 the engines formed a core part of the restoration but a system was needed to revive the engines without resort to wasting water. The present hydraulic system makes use of an electrically driven centrifugal water pump, regulated via various pressure bypass valves and safety valves, to draw and pressurise water from a large holding tank and feed it, via the original control valves, to the engines. The exhaust water is then simply fed back to the holding tank. In order to explain how the hydraulics works in a more intuitive way, a video and a text description should be offered in the application.

The next section will discuss how to meet the above requirements from a technical perspective.

\section{System specifications}
\subsection{The platform chosen}
The main reason for building the application as a web app is that the cross-platform HTML content can reduces the cost of the project by being shared among different platforms such as iOS and Android. Moreover, the content can be mobile optimised, which provides an appropriately sized and touchscreen-enabled experience for mobile visitors. Table~\ref{ta:appCompare} presents some of other differences between native apps and web apps. 
\begin{table}
\begin{center}
\begin{tabularx}{\linewidth}{ l X X }
\hline
\textbf{The issues} & \textbf{Web app} & \textbf{Native app} \\
\hline
Hardware integration & Access through the browser is limited. Access to geolocation can be made by Javascript. & It has all the access including camera, microphone, accelerometer, etc.\\
\hline
Distribution & It can be accessed through URL. It can behave like a native app by being bookmarked to home screen on iOS. & It should be downloaded through app stores, and most app stores require approval.\\
\hline
Performance & The device has to interpret the code into native code at run time, which takes time especially for multiple animations. & The code is pre-compiled, therefore high performance can be fast without interpretation once the app runs.\\
\hline
Cross-platform & It can share the same code across multiple platforms. CSS can specify the size once the size of screen changes. & Code is written in a specific language for each platform. It has to be installed first before running.\\
\hline
Updates & As all the contents are saved on the server, users can update an app just like refreshing a website. & Users need to download the latest version through the app stores.\\
\hline
\end{tabularx}
\caption{Web app vs. native app} \label{ta:appCompare} 
\end{center}
\end{table}
As the app for the Organ Project aims to be shared with more users, and all of the required functions and effects can be achieved by some of the existing well-developed APIs and libraries, which will be discussed hereafter, the app in this report is developed based on the web platform and named as \enquote{The Organ Web App}.

\subsection{Web audio API}
The fifth version of the HTML standard adds many new syntactic features, among which the \verb|<audio>| element provides native support for audio playback in all modern browsers \cite{smus2013web}. However, there are significant limitations of the \verb|<audio>| element such as no way to pre-buffer a sound nor analyse sounds. In order to provide a versatile system for controlling audio on the web, the Web Audio API is developed, which is completely separate from the <audio> tag. As a high-level JavaScript API, it is widely used in games, interactive applications and music synthesis applications. Therefore, the Web Audio API is applied in The Organ Web App for allowing users to hear different sounds when playing the keyboard in the app, which features pre-recorded audio samples of six timbres from the organ.

The \verb|AudioContex| is the main concept for building the Web Audio API. It shapes the audio graph by defining how the audio modules linked together from its source to its destination. Each module is represented by an \verb|AudioNode|. The node's properties can be modified when the audio passes through. The following code presents how to initialise an \verb|AudioContex|.
\begin{verbatim}
var contextClass = (window.AudioContext || 
  window.webkitAudioContext || 
  window.mozAudioContext || 
  window.oAudioContext || 
  window.msAudioContext);
if (contextClass) {
  // Web Audio API is available.
  var context = new contextClass();
} else {
  // Ask the user to use a supported browser if it is not available.
  alert('Web Audio API is not available in your browser.');
}
\end{verbatim}

The \verb|AudioBuffer| is used to cache the audio samples, which can be multiple formats such as WAV, MP3 and OGG. As \verb|XMLHttpRequest| makes sending HTTP requests very easy, it is also used to load an audio sample into the Web Audio API. As the audio sample is a binary file, the \verb|responseType| should be set as \verb|arraybuffer|, which will be processed by \verb|decodeAudioData|. This all happens asynchronously and doesn?t block the main UI thread. The following code presents how to load an organ audio sample.
\begin{verbatim}
var organSampleBuffer = null;
var context = new webkitAudioContext();
 
function loadOrganSample(url) {
  var request = new XMLHttpRequest();
  request.open('GET', url, true);
  request.responseType = 'arraybuffer';
 
  // Decode asynchronously
  request.onload = function() {
    context.decodeAudioData(request.response, function(buffer) {
      organSampleBuffer = buffer;
    }, onError);
  }
  request.send();
}
\end{verbatim}

Once the \verb|AudioBuffer| has been loaded, a source node can be created and connected into the audio graph. Then call \verb|start(0)| to play the sound. Supposed the above organ audio sample has been loaded to the buffer, the following code shows how to play the audio.
\begin{verbatim}
function playSound(buffer) {
  var source = context.createBufferSource();
  source.buffer = buffer;
  source.connect(context.destination);
  source.start(0);
}
\end{verbatim}

The \verb|playSound| function can be called anytime, for instance, mouse clicking or keyboard pressing. The Web Audio API also allows developers to change basic properties of the audio, such as timing and loudness, visualise the audio, apply spatial effects and much more.

\subsection{JavaScript libraries}
To deal with the complex handling of browser differences and some other difficulties considering advanced JavaScript programming, a lot of JavaScript libraries have been developed. In our 

\section{Prototype}
\subsection{Paper prototype}
\subsection{Design process}
\subsection{Implementation}
\subsection{Testing}